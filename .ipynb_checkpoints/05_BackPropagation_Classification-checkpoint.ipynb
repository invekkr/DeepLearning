{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182dc294-3e72-4459-bd71-c26ecede4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9aa11a1-b978-4098-8737-ff497234f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[8,8,4],[7,9,5],[6,10,6],[5,12,7]], columns=['cgpa', 'profile_score', 'lpa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "274d564f-27c7-484d-b58a-0e086f3632da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cgpa</th>\n",
       "      <th>profile_score</th>\n",
       "      <th>lpa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cgpa  profile_score  lpa\n",
       "0     8              8    4\n",
       "1     7              9    5\n",
       "2     6             10    6\n",
       "3     5             12    7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14663bb8-a338-4e7b-8b6f-4044c8029af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "  \n",
    "  np.random.seed(3)\n",
    "  parameters = {}\n",
    "  L = len(layer_dims)         \n",
    "\n",
    "  for l in range(1, L):\n",
    "\n",
    "    parameters['W' + str(l)] = np.ones((layer_dims[l-1], layer_dims[l]))*0.1\n",
    "    parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "      \n",
    "\n",
    "  return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace0a7f-f570-4120-a974-8e5a93a1a92f",
   "metadata": {},
   "source": [
    "### Function: `initialize_parameters`\n",
    "\n",
    "This function initializes the **weights and biases** for a fully connected neural network.\n",
    "\n",
    "#### Example call\n",
    "```python\n",
    "initialize_parameters([2, 2, 1])\n",
    "Meaning of [2, 2, 1]\n",
    "Input layer: 2 neurons\n",
    "\n",
    "Hidden layer: 2 neurons\n",
    "\n",
    "Output layer: 1 neuron\n",
    "\n",
    "How the function works\n",
    "layer_dims defines the network structure\n",
    "\n",
    "L = len(layer_dims) gives the number of layers\n",
    "\n",
    "The loop runs L - 1 times (input layer has no weights)\n",
    "\n",
    "For [2, 2, 1], the loop runs 2 times:\n",
    "\n",
    "Creates W1 (2√ó2) and b1 (2√ó1)\n",
    "\n",
    "Creates W2 (2√ó1) and b2 (1√ó1)\n",
    "\n",
    "Initialization values\n",
    "All weights are initialized to 0.1\n",
    "\n",
    "All biases are initialized to 0\n",
    "\n",
    "One-line summary\n",
    "This function creates all the weights and biases with correct shapes so that a neural network can start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8cb3bd7-1ff0-40d4-9ae0-e9adb3286c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.1, 0.1],\n",
       "        [0.1, 0.1]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[0.1],\n",
       "        [0.1]]),\n",
       " 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_parameters([2,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7972dfea-41bd-47dc-bd82-29e0068abd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "  \n",
    "  Z = np.dot(W.T, A_prev) + b\n",
    "  \n",
    "  return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b5a18-6378-4a28-b731-31a21843dcfd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This function computes the **linear part of forward propagation** for one layer in a neural network.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Purpose\n",
    "\n",
    "It calculates the weighted sum of inputs plus bias **before applying the activation function**.\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical representation\n",
    "\n",
    "[\n",
    "Z = W^T A_{prev} + b\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### Parameters\n",
    "\n",
    "* **A_prev**: Activations (or inputs) from the previous layer\n",
    "* **W**: Weight matrix of the current layer\n",
    "* **b**: Bias vector of the current layer\n",
    "\n",
    "---\n",
    "\n",
    "### Shape intuition\n",
    "\n",
    "If:\n",
    "\n",
    "* `A_prev` has shape `(n_prev, m)`\n",
    "* `W` has shape `(n_prev, n_current)`\n",
    "* `b` has shape `(n_current, 1)`\n",
    "\n",
    "Then:\n",
    "\n",
    "* `Z` has shape `(n_current, m)`\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "For 2 inputs and 1 neuron:\n",
    "\n",
    "```python\n",
    "A_prev = [[x1],\n",
    "          [x2]]\n",
    "\n",
    "W = [[w1],\n",
    "     [w2]]\n",
    "\n",
    "b = [[b]]\n",
    "```\n",
    "\n",
    "[\n",
    "Z = w_1x_1 + w_2x_2 + b\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### One-line summary\n",
    "\n",
    "This function computes the linear transformation ( Z ) used in forward propagation of a neural network.\n",
    "\n",
    "```\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54998ef9-45d5-461e-ab23-bba192721e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Prop\n",
    "def L_layer_forward(X, parameters):\n",
    "\n",
    "  A = X\n",
    "  L = len(parameters) // 2                  # number of layers in the neural network\n",
    "  \n",
    "  for l in range(1, L+1):\n",
    "    A_prev = A \n",
    "    Wl = parameters['W' + str(l)]\n",
    "    bl = parameters['b' + str(l)]\n",
    "    #print(\"A\"+str(l-1)+\": \", A_prev)\n",
    "    #print(\"W\"+str(l)+\": \", Wl)\n",
    "    #print(\"b\"+str(l)+\": \", bl)\n",
    "    #print(\"--\"*20)\n",
    "\n",
    "    A = linear_forward(A_prev, Wl, bl)\n",
    "    #print(\"A\"+str(l)+\": \", A)\n",
    "    #print(\"**\"*20)\n",
    "          \n",
    "  return A,A_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c3146e-f2af-4735-bfee-461f2e6e4cc5",
   "metadata": {},
   "source": [
    "````markdown\n",
    "## Function: `L_layer_forward`\n",
    "\n",
    "This function performs **forward propagation through all layers** of a neural network.\n",
    "\n",
    "---\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Forward Prop\n",
    "def L_layer_forward(X, parameters):\n",
    "\n",
    "    A = X\n",
    "    L = len(parameters) // 2   # number of layers in the neural network\n",
    "  \n",
    "    for l in range(1, L+1):\n",
    "        A_prev = A \n",
    "        Wl = parameters['W' + str(l)]\n",
    "        bl = parameters['b' + str(l)]\n",
    "\n",
    "        A = linear_forward(A_prev, Wl, bl)\n",
    "          \n",
    "    return A, A_prev\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "It passes the input data **layer by layer** through the network and computes the final output.\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-step explanation\n",
    "\n",
    "#### 1. Input initialization\n",
    "\n",
    "```python\n",
    "A = X\n",
    "```\n",
    "\n",
    "* `X` is the input data\n",
    "* `A` stores the activations (initially, input itself)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Number of layers\n",
    "\n",
    "```python\n",
    "L = len(parameters) // 2\n",
    "```\n",
    "\n",
    "* Each layer has `W` and `b`\n",
    "* Total layers = number of parameter pairs\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "parameters = {W1, b1, W2, b2}\n",
    "L = 2\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Loop through layers\n",
    "\n",
    "```python\n",
    "for l in range(1, L+1):\n",
    "```\n",
    "\n",
    "* Runs once per layer\n",
    "* `l = 1` ‚Üí first hidden layer\n",
    "* `l = L` ‚Üí output layer\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Store previous activation\n",
    "\n",
    "```python\n",
    "A_prev = A\n",
    "```\n",
    "\n",
    "* Saves activation from the previous layer\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Extract parameters\n",
    "\n",
    "```python\n",
    "Wl = parameters['W' + str(l)]\n",
    "bl = parameters['b' + str(l)]\n",
    "```\n",
    "\n",
    "* Gets weights and biases for layer `l`\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Linear forward step\n",
    "\n",
    "```python\n",
    "A = linear_forward(A_prev, Wl, bl)\n",
    "```\n",
    "\n",
    "* Computes:\n",
    "  [\n",
    "  A = W_l^T A_{prev} + b_l\n",
    "  ]\n",
    "* Updates activation for the current layer\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. Return values\n",
    "\n",
    "```python\n",
    "return A, A_prev\n",
    "```\n",
    "\n",
    "* `A` ‚Üí final output of the network\n",
    "* `A_prev` ‚Üí activation of the second-last layer\n",
    "\n",
    "---\n",
    "\n",
    "### One-line summary\n",
    "\n",
    "This function performs forward propagation across all layers of the neural network and returns the final output.\n",
    "\n",
    "```\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d4b5e2d-ef36-4aa1-a62f-9b6787e5013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[0].reshape(2,1) # Shape(no of features, no. of training example)\n",
    "y = df[['lpa']].values[0][0]\n",
    "\n",
    "# Parameter initialization\n",
    "parameters = initialize_parameters([2,2,1])\n",
    "\n",
    "y_hat,A1 = L_layer_forward(X, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f6ce1f-f42e-4696-8dd8-df54cdb75694",
   "metadata": {},
   "source": [
    "\n",
    "## Example: Forward Propagation with Real Data\n",
    "\n",
    "This code shows how **input data is passed through a neural network** using the functions you defined.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Input data `X`\n",
    "\n",
    "* You select **2 features**:\n",
    "\n",
    "  * `cgpa`\n",
    "  * `profile_score`\n",
    "* `.values[0]` ‚Üí takes the **first training example**\n",
    "* `.reshape(2,1)` ‚Üí converts it to a column vector\n",
    "\n",
    "üìê Shape of `X`:\n",
    "\n",
    "```\n",
    "(2, 1)\n",
    "```\n",
    "\n",
    "Meaning:\n",
    "\n",
    "* 2 features\n",
    "* 1 training example\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: True output `y`\n",
    "\n",
    "```python\n",
    "y = df[['lpa']].values[0][0]\n",
    "```\n",
    "\n",
    "* Extracts the **actual label (target value)** for the same data point\n",
    "* This is the **ground truth** used later to compute loss\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Initialize parameters\n",
    "\n",
    "```python\n",
    "parameters = initialize_parameters([2,2,1])\n",
    "```\n",
    "\n",
    "Creates weights and biases for a network with:\n",
    "\n",
    "```\n",
    "2 inputs ‚Üí 2 hidden neurons ‚Üí 1 output neuron\n",
    "```\n",
    "\n",
    "Parameters created:\n",
    "\n",
    "* `W1 (2√ó2)`, `b1 (2√ó1)`\n",
    "* `W2 (2√ó1)`, `b2 (1√ó1)`\n",
    "\n",
    "All weights = `0.1`, all biases = `0`\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Forward propagation\n",
    "\n",
    "```python\n",
    "y_hat, A1 = L_layer_forward(X, parameters)\n",
    "```\n",
    "\n",
    "What happens internally:\n",
    "\n",
    "1. **Layer 1 (hidden layer)**\n",
    "   [\n",
    "   A_1 = W_1^T X + b_1\n",
    "   ]\n",
    "\n",
    "2. **Layer 2 (output layer)**\n",
    "   [\n",
    "   \\hat{y} = W_2^T A_1 + b_2\n",
    "   ]\n",
    "\n",
    "---\n",
    "\n",
    "### Returned values\n",
    "\n",
    "* `y_hat` ‚Üí **predicted output** (model prediction)\n",
    "* `A1` ‚Üí **activation from hidden layer**\n",
    "\n",
    "---\n",
    "\n",
    "### Shape intuition\n",
    "\n",
    "| Variable | Shape |\n",
    "| -------- | ----- |\n",
    "| `X`      | (2,1) |\n",
    "| `A1`     | (2,1) |\n",
    "| `y_hat`  | (1,1) |\n",
    "\n",
    "---\n",
    "\n",
    "### One-line summary\n",
    "\n",
    "This code takes one data point, initializes a 2‚Äì2‚Äì1 neural network, and performs forward propagation to compute the predicted output.\n",
    "\n",
    "```\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3375410-99fe-4349-8504-a2bbdf48b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = y_hat[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c4491-8485-4878-91aa-9d1d890ba921",
   "metadata": {},
   "source": [
    "\n",
    "### Line: `y_hat = y_hat[0][0]`\n",
    "\n",
    "This line converts the model output from a **2D NumPy array** into a **single scalar value**.\n",
    "\n",
    "---\n",
    "\n",
    "### Before this line\n",
    "\n",
    "After forward propagation:\n",
    "\n",
    "```python\n",
    "y_hat, A1 = L_layer_forward(X, parameters)\n",
    "````\n",
    "\n",
    "`y_hat` looks like this:\n",
    "\n",
    "```text\n",
    "[[value]]\n",
    "```\n",
    "\n",
    "üìê Shape:\n",
    "\n",
    "```\n",
    "(1, 1)\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "y_hat = [[0.85]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why this happens\n",
    "\n",
    "Neural networks work with **matrices**, even when:\n",
    "\n",
    "* There is only **one output neuron**\n",
    "* There is only **one training example**\n",
    "\n",
    "So the output is kept as a `(1,1)` array.\n",
    "\n",
    "---\n",
    "\n",
    "### What `[0][0]` does\n",
    "\n",
    "```python\n",
    "y_hat = y_hat[0][0]\n",
    "```\n",
    "\n",
    "* First `[0]` ‚Üí selects the first row\n",
    "* Second `[0]` ‚Üí selects the first column\n",
    "\n",
    "Result:\n",
    "\n",
    "```python\n",
    "y_hat = 0.85\n",
    "```\n",
    "\n",
    "Now `y_hat` is a **scalar**, not an array.\n",
    "\n",
    "---\n",
    "\n",
    "### Why convert to scalar?\n",
    "\n",
    "* Easier to:\n",
    "\n",
    "  * Print values\n",
    "  * Compare with `y`\n",
    "  * Compute loss manually\n",
    "* Useful for **single-example predictions**\n",
    "\n",
    "---\n",
    "\n",
    "### One-line summary\n",
    "\n",
    "This line extracts the scalar prediction value from a `(1,1)` NumPy array returned by the neural network.\n",
    "\n",
    "```\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58e83fbf-c4ee-44cb-bde8-5e5aacc7bdbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.6],\n",
       "       [1.6]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d50aa-f782-4372-be0b-4ba66de3f954",
   "metadata": {},
   "source": [
    "\n",
    "## What is `A1`?\n",
    "\n",
    "`A1` is the **activation (output) of the hidden layer** in your neural network.\n",
    "\n",
    "---\n",
    "\n",
    "### Where does `A1` come from?\n",
    "\n",
    "From this line:\n",
    "\n",
    "```python\n",
    "y_hat, A1 = L_layer_forward(X, parameters)\n",
    "````\n",
    "\n",
    "Inside `L_layer_forward`:\n",
    "\n",
    "* `A_prev` is updated at each layer\n",
    "* At the **last iteration**, before computing the output layer,\n",
    "  `A_prev` holds the activation of the **hidden layer**\n",
    "* That value is returned as `A1`\n",
    "\n",
    "---\n",
    "\n",
    "### In your network structure\n",
    "\n",
    "```python\n",
    "initialize_parameters([2, 2, 1])\n",
    "```\n",
    "\n",
    "Architecture:\n",
    "\n",
    "```\n",
    "Input (2) ‚Üí Hidden (2) ‚Üí Output (1)\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "* `A1` = output of the **hidden layer**\n",
    "* `y_hat` = output of the **final layer**\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical meaning\n",
    "\n",
    "Hidden layer computation:\n",
    "\n",
    "[\n",
    "A_1 = W_1^T X + b_1\n",
    "]\n",
    "\n",
    "(no activation function applied yet in your code)\n",
    "\n",
    "---\n",
    "\n",
    "### Shape of `A1`\n",
    "\n",
    "```text\n",
    "A1 shape = (2, 1)\n",
    "```\n",
    "\n",
    "Why?\n",
    "\n",
    "* 2 hidden neurons\n",
    "* 1 training example\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "A1 =\n",
    "[[a1],\n",
    " [a2]]\n",
    "```\n",
    "\n",
    "Each value corresponds to **one hidden neuron‚Äôs output**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why `A1` is useful\n",
    "\n",
    "* Needed for **backpropagation**\n",
    "* Used to compute gradients for `W2`\n",
    "* Helps understand what the hidden layer learned\n",
    "\n",
    "---\n",
    "\n",
    "### One-line summary\n",
    "\n",
    "`A1` is the hidden layer‚Äôs output (activation) produced during forward propagation, and it is later used for backpropagation.\n",
    "\n",
    "```\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6138895-5c46-4d57-ae31-46345a6106e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, y, y_hat, A1, X):\n",
    "\n",
    "    lr = 0.001\n",
    "    error_grad = 2 * (y - y_hat)\n",
    "\n",
    "    # ----- Update W2 -----\n",
    "    parameters['W2'][0][0] += lr * error_grad * A1[0][0]\n",
    "    parameters['W2'][1][0] += lr * error_grad * A1[1][0]\n",
    "\n",
    "    # ----- Update b2 -----\n",
    "    parameters['b2'][0][0] += lr * error_grad\n",
    "\n",
    "\n",
    "    # ----- Update W1 (hidden neuron 1) -----\n",
    "    parameters['W1'][0][0] += lr * error_grad * parameters['W2'][0][0] * X[0][0]\n",
    "    parameters['W1'][0][1] += lr * error_grad * parameters['W2'][0][0] * X[1][0]\n",
    "    parameters['b1'][0][0] += lr * error_grad * parameters['W2'][0][0]\n",
    "\n",
    "\n",
    "    # ----- Update W1 (hidden neuron 2) -----\n",
    "    parameters['W1'][1][0] += lr * error_grad * parameters['W2'][1][0] * X[0][0]\n",
    "    parameters['W1'][1][1] += lr * error_grad * parameters['W2'][1][0] * X[1][0]\n",
    "    parameters['b1'][1][0] += lr * error_grad * parameters['W2'][1][0]\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b8d08-6554-40a9-985d-bf55ecae8650",
   "metadata": {},
   "source": [
    "\n",
    "## Explanation of `update_parameters` (Corrected Version)\n",
    "\n",
    "This function performs **manual backpropagation** and updates the **weights and biases** of a  \n",
    "**2 ‚Üí 2 ‚Üí 1 neural network** using **one training example**.\n",
    "\n",
    "---\n",
    "\n",
    "## Network Setup\n",
    "- Architecture: `2 ‚Üí 2 ‚Üí 1`\n",
    "- Loss function: Mean Squared Error (MSE)\n",
    "\\[\n",
    "L = (y - \\hat{y})^2\n",
    "\\]\n",
    "- Learning rate: `0.001`\n",
    "- No activation functions (pure linear model)\n",
    "\n",
    "---\n",
    "\n",
    "## Step 0: Error term\n",
    "\n",
    "```python\n",
    "lr = 0.001\n",
    "error_grad = 2 * (y - y_hat)\n",
    "````\n",
    "\n",
    "From MSE:\n",
    "[\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} = 2 (y - \\hat{y})\n",
    "]\n",
    "\n",
    "This value tells **how wrong the prediction is** and in which direction to update.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Update Output Layer Parameters (`W2`, `b2`)\n",
    "\n",
    "### Update `W2`\n",
    "\n",
    "```python\n",
    "parameters['W2'][0][0] += lr * error_grad * A1[0][0]\n",
    "parameters['W2'][1][0] += lr * error_grad * A1[1][0]\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "[\n",
    "\\frac{\\partial L}{\\partial W_2} = 2 (y - \\hat{y}) \\cdot A_1\n",
    "]\n",
    "\n",
    "Each output weight is updated using:\n",
    "\n",
    "* Prediction error\n",
    "* Corresponding hidden neuron output\n",
    "\n",
    "---\n",
    "\n",
    "### Update `b2`\n",
    "\n",
    "```python\n",
    "parameters['b2'][0][0] += lr * error_grad\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "[\n",
    "\\frac{\\partial L}{\\partial b_2} = 2 (y - \\hat{y})\n",
    "]\n",
    "\n",
    "Bias depends **only on the error**, not on inputs or weights.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Update Hidden Layer Parameters (`W1`, `b1`)\n",
    "\n",
    "Here we apply the **chain rule**.\n",
    "\n",
    "[\n",
    "\\frac{\\partial L}{\\partial W_1}\n",
    "===============================\n",
    "\n",
    "\\frac{\\partial L}{\\partial \\hat{y}}\n",
    "\\cdot\n",
    "\\frac{\\partial \\hat{y}}{\\partial A_1}\n",
    "\\cdot\n",
    "\\frac{\\partial A_1}{\\partial W_1}\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### Update `W1` (Hidden Neuron 1)\n",
    "\n",
    "```python\n",
    "parameters['W1'][0][0] += lr * error_grad * parameters['W2'][0][0] * X[0][0]\n",
    "parameters['W1'][0][1] += lr * error_grad * parameters['W2'][0][0] * X[1][0]\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Error flows back through `W2`\n",
    "* Then through the input `X`\n",
    "\n",
    "---\n",
    "\n",
    "### Update `b1` (Hidden Neuron 1)\n",
    "\n",
    "```python\n",
    "parameters['b1'][0][0] += lr * error_grad * parameters['W2'][0][0]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Update `W1` (Hidden Neuron 2)\n",
    "\n",
    "```python\n",
    "parameters['W1'][1][0] += lr * error_grad * parameters['W2'][1][0] * X[0][0]\n",
    "parameters['W1'][1][1] += lr * error_grad * parameters['W2'][1][0] * X[1][0]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Update `b1` (Hidden Neuron 2)\n",
    "\n",
    "```python\n",
    "parameters['b1'][1][0] += lr * error_grad * parameters['W2'][1][0]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Assumptions\n",
    "\n",
    "* Single training example\n",
    "* Linear layers only\n",
    "* Manual gradient computation\n",
    "* Hardcoded for `2 ‚Üí 2 ‚Üí 1`\n",
    "\n",
    "---\n",
    "\n",
    "## One-line Summary\n",
    "\n",
    "This function manually applies **backpropagation using the chain rule** and updates all weights and biases of a **2‚Äì2‚Äì1 neural network** to reduce prediction error.\n",
    "\n",
    "```\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38ae7117-03ee-41f8-891f-94d876ecf3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.10658137, 0.10658137],\n",
       "        [0.10658137, 0.10658137]]),\n",
       " 'b1': array([[0.00082267],\n",
       "        [0.00082267]]),\n",
       " 'W2': array([[0.111776],\n",
       "        [0.111776]]),\n",
       " 'b2': array([[0.00736]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_parameters(parameters,y,y_hat,A1,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "162b9d1a-937e-423b-a39a-e6539644b43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.10658137, 0.10658137],\n",
       "        [0.10658137, 0.10658137]]),\n",
       " 'b1': array([[0.00082267],\n",
       "        [0.00082267]]),\n",
       " 'W2': array([[0.111776],\n",
       "        [0.111776]]),\n",
       " 'b2': array([[0.00736]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[0].reshape(2,1) # Shape(no of features, no. of training example)\n",
    "y = df[['lpa']].values[0][0]\n",
    "\n",
    "# Parameter initialization\n",
    "parameters = initialize_parameters([2,2,1])\n",
    "\n",
    "y_hat,A1 = L_layer_forward(X,parameters)\n",
    "y_hat = y_hat[0][0]\n",
    "\n",
    "update_parameters(parameters,y,y_hat,A1,X)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ab031b4-110d-4bce-8061-50f80803d23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.11481311, 0.11716504],\n",
       "        [0.11481311, 0.11716504]]),\n",
       " 'b1': array([[0.00199863],\n",
       "        [0.00199863]]),\n",
       " 'W2': array([[0.12751067],\n",
       "        [0.12751067]]),\n",
       " 'b2': array([[0.01658246]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[1].reshape(2,1) # Shape(no of features, no. of training exaplme)\n",
    "y = df[['lpa']].values[1][0]\n",
    "\n",
    "y_hat,A1 = L_layer_forward(X,parameters)\n",
    "y_hat = y_hat[0][0]\n",
    "\n",
    "update_parameters(parameters,y,y_hat,A1,X)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83aa289e-35c3-4579-98f3-19b30c745c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.12458335, 0.13344878],\n",
       "        [0.12461077, 0.13349447]]),\n",
       " 'b1': array([[0.00362701],\n",
       "        [0.00363158]]),\n",
       " 'W2': array([[0.1477752 ],\n",
       "        [0.14818986]]),\n",
       " 'b2': array([[0.02760173]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[2].reshape(2,1) # Shape(no of features, no. of training exaplme)\n",
    "y = df[['lpa']].values[2][0]\n",
    "\n",
    "y_hat,A1 = L_layer_forward(X,parameters)\n",
    "y_hat = y_hat[0][0]\n",
    "\n",
    "update_parameters(parameters,y,y_hat,A1,X)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c69742d1-b123-4aed-b051-78fd558b0fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.13562189, 0.15994127],\n",
       "        [0.13579618, 0.16033944]]),\n",
       " 'b1': array([[0.00583472],\n",
       "        [0.00586866]]),\n",
       " 'W2': array([[0.17460429],\n",
       "        [0.1769274 ]]),\n",
       " 'b2': array([[0.04024579]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[3].reshape(2,1) # Shape(no of features, no. of training exaplme)\n",
    "y = df[['lpa']].values[3][0]\n",
    "\n",
    "y_hat,A1 = L_layer_forward(X,parameters)\n",
    "y_hat = y_hat[0][0]\n",
    "\n",
    "update_parameters(parameters,y,y_hat,A1,X)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7894c7b9-ae41-450b-939e-e350039f31e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  1 Loss -  26.28249792398698\n",
      "Epoch -  2 Loss -  19.438253848220803\n",
      "Epoch -  3 Loss -  10.139874435827522\n",
      "Epoch -  4 Loss -  3.385561305106485\n",
      "Epoch -  5 Loss -  1.3198454128484565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.273603  , 0.3993222 ],\n",
       "        [0.28787155, 0.42586102]]),\n",
       " 'b1': array([[0.02885522],\n",
       "        [0.03133223]]),\n",
       " 'W2': array([[0.42574893],\n",
       "        [0.50219328]]),\n",
       " 'b2': array([[0.11841278]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epochs implementation\n",
    "\n",
    "parameters = initialize_parameters([2,2,1])\n",
    "epochs = 5\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "  Loss = []\n",
    "\n",
    "  for j in range(df.shape[0]):\n",
    "\n",
    "    X = df[['cgpa', 'profile_score']].values[j].reshape(2,1) # Shape(no of features, no. of training example)\n",
    "    y = df[['lpa']].values[j][0]\n",
    "\n",
    "    # Parameter initialization\n",
    "\n",
    "\n",
    "    y_hat,A1 = L_layer_forward(X,parameters)\n",
    "    y_hat = y_hat[0][0]\n",
    "\n",
    "    update_parameters(parameters,y,y_hat,A1,X)\n",
    "\n",
    "    Loss.append((y-y_hat)**2)\n",
    "\n",
    "  print('Epoch - ',i+1,'Loss - ',np.array(Loss).mean())\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ace46c-1605-4d93-a9dc-05d61bd49dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
